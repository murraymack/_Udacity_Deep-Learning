{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "human_path = \"data/lfw/*/*\"\n",
    "dog_path = \"data/dog_images/*/*/*\"\n",
    "\n",
    "human_files = np.array(glob( human_path ))\n",
    "dog_files = np.array(glob( dog_path ))\n",
    "\n",
    "print('%d Total human images.' % len(human_files))\n",
    "print('%d Total dog images.' % len(dog_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(glob(\"data/lfw/*/*\")))\n",
    "print(human_files[0:5])\n",
    "print(dog_files[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Humans\n",
    "[Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "img = cv2.imread(human_files[15]) # <class 'numpy.ndarray'>\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # convert image to grayscale\n",
    "faces = face_cascade.detectMultiScale(gray) # numpy array of detected faces in grayscale\n",
    "\n",
    "print('Number of faces detected: {}'.format(len(faces)))\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(img, (x,y), (x+w, y+h), (255,0,0), 2) # (image, start point, end point, color, pixel thickness)\n",
    "\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Face Detector\n",
    "Function to return True if human face is detected, else return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0\n",
    "\n",
    "print(face_detector(human_files[10]))\n",
    "print(face_detector(dog_files[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess the Human Face Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "human_files_short = human_files[:50]\n",
    "dog_files_short = dog_files[:50]\n",
    "\n",
    "human_face_counter = 0\n",
    "dog_face_counter = 0\n",
    "\n",
    "dog_and_human = []\n",
    "\n",
    "def haar_tester():\n",
    "\n",
    "    for i in tqdm(range(len(human_files_short))):\n",
    "        human_face = face_detector(human_files_short[i])\n",
    "        dog_face = face_detector(dog_files_short[i])\n",
    "        human_face_counter += human_face\n",
    "        dog_face_counter += dog_face\n",
    "\n",
    "        if dog_face == True:\n",
    "            dog_and_human.append(i)\n",
    "    \n",
    "    print(\"Human Imgs Face Detection %age: \\t{}\".format(human_face_counter / 100))\n",
    "    print(\"Dog Imgs Face Detection %age: \\t{}\".format(dog_face_counter / 100))\n",
    "    print(\"Ten entries from dog_and_human: {}\".format(dog_and_human[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(dog_files_short[15])\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('haarcascades/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "human_face_counter = 0\n",
    "dog_face_counter = 0\n",
    "dog_and_human = []\n",
    "\n",
    "haar_tester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(dog_files_short[1])\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "human_face_counter = 0\n",
    "dog_face_counter = 0\n",
    "dog_and_human = []\n",
    "\n",
    "for i in tqdm(range(len(human_files_short))):\n",
    "    human_face = face_detector(human_files_short[i])\n",
    "    dog_face = face_detector(dog_files_short[i])\n",
    "    human_face_counter += human_face\n",
    "    dog_face_counter += dog_face\n",
    "    \n",
    "    if dog_face == True:\n",
    "        dog_and_human.append(i)\n",
    "    \n",
    "print(\"Human Imgs Face Detection %age: \\t{}\".format(human_face_counter / 50))\n",
    "print(\"Dog Imgs Face Detection %age: \\t{}\".format(dog_face_counter / 50))\n",
    "print(\"Ten entries from dog_and_human: {}\".format(dog_and_human[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Dogs Using Pre-trained VGG16, Standard labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "VGG16 = models.vgg16(pretrained=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    VGG16 = VGG16.cuda()\n",
    "    print(\"VGG16 has been moved to cuda device.\".format(VGG16))\n",
    "\n",
    "for param in VGG16.features.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def VGG16_predict(img_path):\n",
    "\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    transform = transforms.Compose([transforms.Resize((256,256)),\n",
    "                                    transforms.CenterCrop(224),\n",
    "                                    transforms.RandomRotation(35),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.47,0.47,0.47),\n",
    "                                                        (0.47,0.47,0.47))])\n",
    "\n",
    "    image_tensor = transform(img)\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        image_tensor = image_tensor.cuda()\n",
    "\n",
    "    prediction = VGG16(image_tensor)\n",
    "    prediction = prediction.cpu()\n",
    "\n",
    "    predicted_class_index = prediction.data.numpy().argmax()\n",
    "\n",
    "    return predicted_class_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def dog_detector(img_path): \n",
    "    #Return true if dog is detected\n",
    "    #Index between 151 and 268 inclusive\n",
    "\n",
    "    predicted_class_index = VGG16_predict(img_path)\n",
    "\n",
    "    if predicted_class_index >= 151 and predicted_class_index <= 268:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = 50\n",
    "dog_counter = []\n",
    "\n",
    "for i in range(test_cases):\n",
    "    dog_detected = dog_detector(dog_files[i])\n",
    "    if dog_detected == True:\n",
    "        dog_counter.append(1)\n",
    "    else:\n",
    "        dog_counter.append(0)\n",
    "\n",
    "result = np.asarray(dog_counter)\n",
    "print(result.mean(), \" Accuracy for 50 test cases\")\n",
    "\n",
    "test_cases = 500\n",
    "dog_counter = []\n",
    "\n",
    "for i in range(test_cases):\n",
    "    dog_detected = dog_detector(dog_files[i])\n",
    "    if dog_detected == True:\n",
    "        dog_counter.append(1)\n",
    "    else:\n",
    "        dog_counter.append(0)\n",
    "\n",
    "result = np.asarray(dog_counter)\n",
    "print(result.mean(), \"Accuracy for 500 test cases\")\n",
    "\n",
    "test_cases = 5000\n",
    "dog_counter = []\n",
    "\n",
    "for i in range(test_cases):\n",
    "    dog_detected = dog_detector(dog_files[i])\n",
    "    if dog_detected == True:\n",
    "        dog_counter.append(1)\n",
    "    else:\n",
    "        dog_counter.append(0)\n",
    "\n",
    "result = np.asarray(dog_counter)\n",
    "print(result.mean(), \"Accuracy for 5000 test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating labels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "def gen_labels(list_of_paths):\n",
    "    df = pd.DataFrame(list_of_paths)\n",
    "    df.rename(columns={0:\"file_path\"}, inplace=True)\n",
    "    df['label'] = None\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if 'lfw' in df.loc[i,'file_path']:\n",
    "            df.loc[i,'label'] = 0\n",
    "        elif 'dog' in df.loc[i,'file_path']:\n",
    "            myString = df.loc[i,'file_path']\n",
    "            if myString.find('\\\\') > 0:\n",
    "                cut1, cut2, cut3, cut4 = myString.split(\"\\\\\")\n",
    "                dogLabel, discard = cut3.split('.')\n",
    "                df.loc[i,'label'] = int(dogLabel)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fill_human_frame(source_df, target_index):\n",
    "    new_df = pd.DataFrame(columns={'file_path','label'})\n",
    "    counter = 0\n",
    "    \n",
    "    for i in target_index:\n",
    "        a, b = source_df.loc[i, ['file_path', 'label']]\n",
    "        new_df.loc[counter,['file_path','label']] = a, b\n",
    "        counter += 1\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def fill_dog_frame(source_df, train_type=None):\n",
    "    new_df = pd.DataFrame(columns={'file_path', 'label'})\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(len(source_df)):\n",
    "        if train_type in source_df.loc[i,'file_path']:\n",
    "            new_df.loc[i,['file_path','label']] = source_df.loc[i]\n",
    "            counter += 0\n",
    "    \n",
    "    new_df.reset_index(inplace=True)\n",
    "    new_df.drop(['index'], axis=1, inplace=True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def shuffle_reset_index(df):\n",
    "    new_df = df\n",
    "    new_df = new_df.sample(frac=1)\n",
    "    new_df = new_df.reset_index()\n",
    "    new_df = new_df.drop(['index'], axis=1)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_paths_all = gen_labels(glob('data/lfw/*/*'))\n",
    "dog_paths_all = gen_labels(glob('data/dog_images/*/*/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create shuffled indices for human images b/c they were not pre-specified as train/valid/test\n",
    "human_indices = list(range(len(human_paths_all)))\n",
    "np.random.shuffle(human_indices)\n",
    "human_train_idx = human_indices[:int(len(human_indices) * 0.8)]\n",
    "x = human_indices[-int(len(human_indices) * 0.2):]\n",
    "human_valid_idx = x[:int(len(x) * 0.5)]\n",
    "human_test_idx = x[-int(len(x) * 0.5):]\n",
    "\n",
    "human_data_train = fill_human_frame(human_paths_all, human_train_idx)\n",
    "human_data_test = fill_human_frame(human_paths_all, human_valid_idx)\n",
    "human_data_valid = fill_human_frame(human_paths_all, human_test_idx)\n",
    "\n",
    "dog_data_train = fill_dog_frame(dog_paths_all, train_type=\"train\")\n",
    "dog_data_valid = fill_dog_frame(dog_paths_all, train_type=\"valid\")\n",
    "dog_data_test = fill_dog_frame(dog_paths_all, train_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/lfw\\Luiz_Inacio_Lula_da_Silva\\Luiz_Inacio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/lfw\\John_Goold\\John_Goold_0001.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/lfw\\George_Robertson\\George_Robertson_001...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/lfw\\Gerhard_Schroeder\\Gerhard_Schroeder_0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/lfw\\Hanns_Schumacher\\Hanns_Schumacher_000...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path label\n",
       "0  data/lfw\\Luiz_Inacio_Lula_da_Silva\\Luiz_Inacio...     0\n",
       "1            data/lfw\\John_Goold\\John_Goold_0001.jpg     0\n",
       "2  data/lfw\\George_Robertson\\George_Robertson_001...     0\n",
       "3  data/lfw\\Gerhard_Schroeder\\Gerhard_Schroeder_0...     0\n",
       "4  data/lfw\\Hanns_Schumacher\\Hanns_Schumacher_000...     0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate human and dog data, shuffle again and reset index.\n",
    "\n",
    "train_data_df = pd.concat([dog_data_train, human_data_train])\n",
    "test_data_df = pd.concat([dog_data_test, human_data_test])\n",
    "valid_data_df = pd.concat([dog_data_valid, human_data_valid])\n",
    "\n",
    "train_data_df = shuffle_reset_index(train_data_df)\n",
    "test_data_df = shuffle_reset_index(test_data_df)\n",
    "valid_data_df = shuffle_reset_index(valid_data_df)\n",
    "\n",
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Data Exploration; Re-map our dog labels with VGG16 imagenet1000 labels\n",
    "https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a#file-imagenet1000_clsidx_to_labels-txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "raw_labels = open('data/imagenet1000_clsidx_to_labels.txt', 'r')\n",
    "raw_labels = list(raw_labels)\n",
    "reference_dog_labels = raw_labels[151:269]\n",
    "print(len(reference_dog_labels))\n",
    "reference_dog_labels[0:5]\n",
    "\n",
    "for i in range(len(reference_dog_labels)):\n",
    "    reference_dog_labels[i] = reference_dog_labels[i].replace(',\\n','')\n",
    "    if reference_dog_labels[i][0] == \" \":\n",
    "        reference_dog_labels[i] = reference_dog_labels[i][1:]\n",
    "\n",
    "ref_dog_dict = {}\n",
    "        \n",
    "for i in reference_dog_labels:\n",
    "    i = i.split(\": \")\n",
    "    i[1] = i[1].replace(\"'\", \"\")\n",
    "    ref_dog_dict[i[0]] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_path_train_check = np.array(glob('data/dog_images/train/*'))\n",
    "dog_path_test_check = np.array(glob('data/dog_images/test/*'))\n",
    "dog_path_valid_check = np.array(glob('data/dog_images/valid/*'))\n",
    "\n",
    "# Function to help clean up the glob path lists\n",
    "def label_list_maker(glob_list):\n",
    "    new_list = []\n",
    "    for item in glob_list:\n",
    "        if item.find('\\\\') != 0:\n",
    "            cut_off = item.find('\\\\') + 1\n",
    "            item = item[cut_off:]\n",
    "        new_list.append(item)  \n",
    "    return new_list\n",
    "\n",
    "# Concatenate all dog glob paths \n",
    "# This will help check for dog breeds which may be included in one data set, but not the other\n",
    "concat =  label_list_maker(dog_path_train_check) + label_list_maker(dog_path_test_check) + label_list_maker(dog_path_valid_check)\n",
    "concat = list(dict.fromkeys(concat)) # Make a set\n",
    "\n",
    "# Instantiate empty dictionary to serve as our dog labels dict\n",
    "# Later we will check to see how this maps to the Imagenet 1000 categories\n",
    "myDog_labels_dict = {}\n",
    "\n",
    "for i in concat:\n",
    "    key = str(i.split(\".\")[0])\n",
    "    value = str(i.split(\".\")[1])\n",
    "    myDog_labels_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ref_dog_dict))\n",
    "print(\"myDog_labels_dict['01']: %s\" % ref_dog_dict['001'])\n",
    "print(\"myDog_labels_dict['118']: %s\" % ref_dog_dict['118'])\n",
    "\n",
    "# Our dog labels dictionary complete:\n",
    "print(len(myDog_labels_dict))\n",
    "print(\"myDog_labels_dict['001']: %s\" % myDog_labels_dict['001'])\n",
    "print(\"myDog_labels_dict['133']: %s\" % myDog_labels_dict['133'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Observation\n",
    "118: Number of dog breed categories in Imagenet 1000<br />\n",
    "133: Number of dog breed categories in our dataset<br />\n",
    "This is likely the reason why VGG16 only achieved 88% accuracy when we ran our dataset on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataloader. Data Input: image_path, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_paths, labels=None, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.to_image = []\n",
    "        self.len = len(self.img_paths)\n",
    "        self.transform = transform\n",
    "        \n",
    "        if labels is not None:\n",
    "            self.labels = labels\n",
    "        else:\n",
    "            self.labels = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        self.img_paths_temp = Image.open(self.img_paths[idx])\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            self.img_paths_temp = transform(self.img_paths_temp)\n",
    "        \n",
    "        X = self.img_paths_temp\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            y = self.labels[idx]\n",
    "        else:\n",
    "            y = 'No Label Found!'\n",
    "        \n",
    "        return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = 0\n",
    "transform = transforms.Compose([transforms.Resize(250),\n",
    "                             transforms.CenterCrop(224),\n",
    "                             transforms.RandomRotation(30),\n",
    "                             transforms.ToTensor()])\n",
    "\n",
    "train_data = CustomDataset(train_data_df['file_path'], train_data_df['label'], transform=transform)\n",
    "valid_data = CustomDataset(valid_data_df['file_path'], valid_data_df['label'], transform=transform)\n",
    "test_data = CustomDataset(test_data_df['file_path'], test_data_df['label'], transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, num_workers=0)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, num_workers=0)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1_1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_3): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_1): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_3): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_1): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (maxPool): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc_1): Linear(in_features=28800, out_features=2880, bias=True)\n",
       "  (fc_2): Linear(in_features=2880, out_features=512, bias=True)\n",
       "  (fc_3): Linear(in_features=512, out_features=134, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(3, 12, 3, stride=1, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(12, 12, 3, stride=1, padding=1)\n",
    "        self.conv1_3 = nn.Conv2d(12, 24, 3, stride=1, padding=1)\n",
    "        self.conv2_1 = nn.Conv2d(24, 48, 3, stride=1, padding=1)\n",
    "        #self.conv2_2 = nn.Conv2d(48, 48, 3, stride=1, padding=1)\n",
    "        self.conv2_3 = nn.Conv2d(48, 96, 3, stride=1, padding=1)\n",
    "        self.conv3_1 = nn.Conv2d(96, 128, 3, stride=1, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        #self.conv3_3 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        #self.conv3_4 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "        self.conv4_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        #self.conv4_2 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
    "        #self.conv4_3 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
    "        \n",
    "        self.maxPool = nn.MaxPool2d(2, 2, 1)\n",
    "        self.dropout = nn.Dropout(p=0.50)\n",
    "        \n",
    "        self.fc_1 = nn.Linear(28800,2880)\n",
    "        self.fc_2 = nn.Linear(2880,512)\n",
    "        self.fc_3 = nn.Linear(512,134)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"Before conv1_1: {}\".format(x.shape))\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = F.relu(self.conv1_3(x))\n",
    "        x = self.maxPool(x)\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        #x = F.relu(self.conv2_2(x))\n",
    "        x = F.relu(self.conv2_3(x))\n",
    "        x = self.maxPool(x)\n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        x = F.relu(self.conv3_2(x))\n",
    "        #x = F.relu(self.conv3_3(x))\n",
    "        #x = F.relu(self.conv3_4(x))\n",
    "        x = self.maxPool(x)\n",
    "        x = F.relu(self.conv4_1(x))\n",
    "        #x = F.relu(self.conv4_2(x))\n",
    "        #x = F.relu(self.conv4_3(x))\n",
    "        x = self.maxPool(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        #print(\"After view, before relu: {}\".format(x.shape))\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc_2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_3(x)\n",
    "        return x\n",
    "    \n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1_1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_3): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_1): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_3): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_1): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (maxPool): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc_1): Linear(in_features=28800, out_features=2880, bias=True)\n",
       "  (fc_2): Linear(in_features=2880, out_features=512, bias=True)\n",
       "  (fc_3): Linear(in_features=512, out_features=134, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "device = \"cuda\"\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainloss: 78.49901580810547\n",
      "trainloss: 148.96137237548828\n",
      "trainloss: 308.73247146606445\n",
      "trainloss: 568.5750999450684\n",
      "trainloss: 1204.1608428955078\n",
      "trainloss: 2313.099245071411\n",
      "Epoch: 1 \tTraining Loss: 2.703021 \tValidation Loss: 2.561246\n",
      "Validation loss decreased (inf --> 2.561246).  Saving model ...\n",
      "trainloss: 49.93525314331055\n",
      "trainloss: 84.97062683105469\n",
      "trainloss: 217.46641540527344\n",
      "trainloss: 402.15576553344727\n",
      "trainloss: 1011.2337760925293\n",
      "trainloss: 1996.0555248260498\n",
      "Epoch: 2 \tTraining Loss: 2.607522 \tValidation Loss: 2.558068\n",
      "Validation loss decreased (2.561246 --> 2.558068).  Saving model ...\n",
      "trainloss: 50.38090896606445\n",
      "trainloss: 84.79438781738281\n",
      "trainloss: 212.38259887695312\n",
      "trainloss: 397.1840057373047\n",
      "trainloss: 993.7785339355469\n",
      "trainloss: 1947.4501132965088\n",
      "Epoch: 3 \tTraining Loss: 2.591977 \tValidation Loss: 2.557171\n",
      "Validation loss decreased (2.558068 --> 2.557171).  Saving model ...\n",
      "trainloss: 49.25724792480469\n",
      "trainloss: 84.53406524658203\n",
      "trainloss: 211.9724578857422\n",
      "trainloss: 398.77682876586914\n",
      "trainloss: 995.8778553009033\n",
      "trainloss: 1938.988452911377\n",
      "Epoch: 4 \tTraining Loss: 2.585773 \tValidation Loss: 2.555959\n",
      "Validation loss decreased (2.557171 --> 2.555959).  Saving model ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 318.00 MiB (GPU 0; 8.00 GiB total capacity; 5.30 GiB already allocated; 101.55 MiB free; 6.17 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d0a481f2de31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\flappybird\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\flappybird\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 318.00 MiB (GPU 0; 8.00 GiB total capacity; 5.30 GiB already allocated; 101.55 MiB free; 6.17 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import ImageFile\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "n_epochs = 10\n",
    "\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    modulus = 0\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "          \n",
    "    # Validate model\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for data, target in valid_loader:\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    " \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_murpher_newfcs.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 76.00 MiB (GPU 0; 8.00 GiB total capacity; 5.80 GiB already allocated; 25.55 MiB free; 6.24 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-deb66aacbb0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\flappybird\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-69f6b25e4eac>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m#x = F.relu(self.conv2_2(x))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2_3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\flappybird\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m    912\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 0; 8.00 GiB total capacity; 5.80 GiB already allocated; 25.55 MiB free; 6.24 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "for data, target in test_loader:\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    print(loss)\n",
    "\n",
    "    valid_loss += loss.item()*data.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
